{
  "hash": "0180ac5fbab23bc92e9c85d35ebb0a21",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Longitudinal Multi-Level Regression\"\nformat:\n  live-html:\n    css: \n      - exams/webex.css\n    include-after-body: exams/webex.js\nwebr:\n  packages:\n    - dplyr\n    - ggplot2\n    - haven\ntoc: true    \n---\n\n\n::: {.cell}\n\n:::\n\n\n\n## Grundidee\nMehrebenenregressionsmodelle (auch Multi-Level-Modelle, Hierachische Modelle oder Mixed-Effects-Modelle) sind ein mächtiges Werkzeug zur Modellierung von Daten, die auf mehreren Ebenen genested (geclusterd) organisiert sind. Diese Cluster können etwa Schülerinnen und Schüler in Klassen sein oder Messwiederholungen in Merkmalsträgern. Die Grundidee der Modellierung längsschnittlicher Daten mit Mehrebenenmodellen besteht darin, sowohl die Variation innerhalb der Cluster (=Merkmalsträger) als auch zwischen den Clustern zu modellieren. Die Variation zischen den Clustern (auch Level-2 Effekte oder Random Effekte) werden dabei als »Streuung von Regressionsparametern« konzeptualisiert. Damit die Zeit in diesem Modellen als Prädiktor berücksichtigt werden kann, müssen die Daten in einer bestimmten Weise strukturiert werden die oft »long data« genannt wird (siehe @tbl-long-data).\n\n\n| ID | Mathe Klasse 1 | Mathe Klasse 2 | Mathe Klasse 3 |\n|----------------|----------------|----------------|----------------|\n| A              | 463            | 475            | 501            |\n| B              | 499            | 503            | 505            |\n| C              | 360            | 365            | 372            |\n\n: »Wide data« {#tbl-wide-data}\n\n\n| ID | Mathe | Klasse |\n|----------------|-------|--------|\n| A              | 463   | 1      |\n| A              | 475   | 2      |\n| A              | 501   | 3      |\n| B              | 499   | 1      |\n| B              | 503   | 2      |\n| B              | 505   | 3      |\n| C              | 360   | 1      |\n| C              | 365   | 2      |\n| C              | 372   | 3      |\n \n: »Long data« {#tbl-long-data}\n\n\n## Data Wrangling {#sec-prädiktion-differenzen}\nFast jede Statistiksoftware verfügt über mehr oder weniger intuitive Werkzeuge zur Datenaufbereitung. In {{< fa-brands:r-project >}} sind insbesondere die Pakete `dplyr` und `tidyr` sehr hilfreich. Hier ein Beispiel für die Warangling von \"Wide\" in \"Long\" Format:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(haven)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(colorspace)\n\ndata_star <- read_sav(\n  \"https://raw.githubusercontent.com/sammerk/WinterSchoolZH/master/data_star_workshop.sav\"\n)\n\ndata_star_subset <- data_star %>%\n  group_by(classID) %>%\n  summarize(\n    MathClassMeanCl0 = mean(gktmathss, na.rm = T),\n    MathClassMeanCl1 = mean(g1tmathss, na.rm = T),\n    MathClassMeanCl2 = mean(g2tmathss, na.rm = T),\n    MathClassMeanCl3 = mean(g3tmathss, na.rm = T),\n    MathClassMeanCl4 = mean(g4tmathss, na.rm = T),\n    PropFreeLunchCl0 = mean(ifelse(gkfreelunch == 1, 1, 0), na.rm = T),\n    PropFreeLunchCl1 = mean(ifelse(g1freelunch == 1, 1, 0), na.rm = T),\n    PropFreeLunchCl2 = mean(ifelse(g2freelunch == 1, 1, 0), na.rm = T),\n    PropFreeLunchCl3 = mean(ifelse(g3freelunch == 1, 1, 0), na.rm = T),\n    ClasssizeCl0 = mean(gkclasssize, na.rm = T),\n    ClasssizeCl1 = mean(g1classsize, na.rm = T),\n    ClasssizeCl2 = mean(g2classsize, na.rm = T),\n    ClasssizeCl3 = mean(g3classsize, na.rm = T)\n  ) %>%\n  ungroup()\n\ndata_star_subset_long <- data_star_subset %>%\n  # pivoting\n  pivot_longer(\n    cols = c(\n      starts_with(\"MathClassMean\"),\n      starts_with(\"PropFreeLunch\"),\n      starts_with(\"Classsize\")\n    ),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %>%\n  # separating variable names\n  mutate(\n    time = as.numeric(str_sub(variable, -1, -1)),\n    variable = str_sub(variable, 1, -4)\n  ) %>%\n  # pivot wider in tidy data format  \n  pivot_wider(\n    names_from = variable,\n    values_from = value\n  )\n```\n:::\n\n\n### Grafische Darstellung\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_sample_naomit <- \n  data_star_subset_long %>% \n  na.omit() %>% \n  group_by(classID) %>% \n  mutate(npergroup = n()) %>% \n  ungroup() %>% \n  filter(npergroup == 3) %>% \n  arrange(classID) %>% \n  slice(31:61)\n\nggplot(data_sample_naomit, aes(x = time, y = MathClassMean)) +\n  geom_point(aes(color = as.factor(classID))) +\n  stat_smooth(\n    aes(color = as.factor(classID)),\n    method = \"lm\",\n    se = FALSE,\n    linewidth = .4\n  ) +\n  stat_smooth(\n    method = \"lm\",\n    se = FALSE,\n    linewidth = 2,\n    linetype = \"dashed\",\n    color = \"#267326\"\n  ) +\n  scale_color_discrete_qualitative(palette = \"Dark3\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Longitudinal-Multi-Level-Regression_files/figure-html/unnamed-chunk-3-1.png){width=2100}\n:::\n:::\n\n\nIn Abbildung XX sehen wir die Entwicklung der Mathematik-Klassenmittelwerte über die Zeit, wobei die einzelnen Klassen durch unterschiedliche Farben dargestellt sind und jede Klasse eine eigene Regressionsgerade hat (No Pooling Modell). Die gestrichelte Linie repräsentiert ein lineares Regressionsmodell für alle Datenpunkte (Complete Pooling Modell).\n\n::: {.callout-caution}\nAchtung: Die Inferenzstatistik des Complete Pooling Modells bedarf besonderer Aufmerksamkeit. Die »klassische« Berechnung von p-Werten und Konfidenzintervallen setzt eine Unabhängigkeit der Residuen voraus. Diese ist hier aber nicht gegeben, denn z.B. starke Klassen werden etwa tendenziell positive Residuen haben.\n:::\n\nEin Mehrebenenmodell stellt eine Art Kompromiss aus No Pooling und Complete Pooling dar und wird daher auch Partial Poolign Modell genannt. Es lässt zwarfür jeden Merkmalsträger eine individuelle Regressionsgerade zu, berücksichtigt bei deren Wahl aber auch Information aus den anderen Merkmalsträgern (sog. Shrinkage). Die Variation zwischen den einzelnen Klassen kann sich auf alle Parameter (etwa Intercept und/oder Slopes) beziehen (siehe Abbildungen).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n```{.r .cell-code}\n# Random Intercept Modell\nmodel_ri <- lmer(MathClassMean ~ time + (1| classID), data = data_sample_naomit)\n\n# Vorhersagen für jede Klasse\npredictions <- data_sample_naomit %>%\n  mutate(predicted = predict(model_ri))\n\n# Plot mit Random Intercept Modell\nggplot(data_sample_naomit, aes(x = time, y = MathClassMean)) +\n  geom_point(aes(color = as.factor(classID))) +\n  geom_line(\n    data = predictions,\n    aes(x = time, y = predicted, color = as.factor(classID)),\n    linewidth = 0.7\n  ) +\n  stat_smooth(method = \"lm\", se = FALSE, linewidth = 2, linetype = \"dashed\", color = \"#267326\") +\n  scale_color_discrete_qualitative(palette = \"Dark3\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Longitudinal-Multi-Level-Regression_files/figure-html/unnamed-chunk-4-1.png){width=2100}\n:::\n\n```{.r .cell-code}\nmodel_rirs <- lmer(MathClassMean ~ time + (1 + time| classID), data = data_sample_naomit)\n\n# Vorhersagen für jede Klasse\npredictions_rirs <- data_sample_naomit %>%\n  mutate(predicted = predict(model_rirs))\n\n# Plot mit Random Intercept Modell\nggplot(data_sample_naomit, aes(x = time, y = MathClassMean)) +\n  geom_point(aes(color = as.factor(classID))) +\n  geom_line(\n    data = predictions_rirs,\n    aes(x = time, y = predicted, color = as.factor(classID)),\n    linewidth = 0.7\n  ) +\n  stat_smooth(method = \"lm\", se = FALSE, linewidth = 2, linetype = \"dashed\", color = \"#267326\") +\n  scale_color_discrete_qualitative(palette = \"Dark3\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Longitudinal-Multi-Level-Regression_files/figure-html/unnamed-chunk-4-2.png){width=2100}\n:::\n:::\n\n\n\nTypischerweise baut man längsschnittliche Mehrebenenmodelle Schritt für Schritt auf und testet jeweils, inwiefern weitere Modellfreiehitsgrade (z.B. varying Slopes) zu zusätzlicher Varianzaufklärung führen.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}